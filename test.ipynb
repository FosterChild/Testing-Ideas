{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDC Strategy for Power BI → Databricks → Purview Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a CDC (Change Data Capture) implementation for syncing Power BI metadata to Microsoft Purview using Delta tables in Databricks.\n",
    "\n",
    "## Key Components:\n",
    "1. **Delta Lake Storage** - Store Power BI metadata with history\n",
    "2. **Change Detection** - Use metadata timestamps (created/updated) to identify changes\n",
    "3. **Incremental Processing** - Only process changed entities\n",
    "4. **Audit Trail** - Track sync status and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "\n",
    "# Delta table paths\n",
    "POWERBI_METADATA_PATH = \"/mnt/delta/powerbi_metadata\"\n",
    "PURVIEW_SYNC_LOG_PATH = \"/mnt/delta/purview_sync_log\"\n",
    "CDC_CHECKPOINT_PATH = \"/mnt/delta/cdc_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d7c41",
   "metadata": {},
   "source": [
    "## Step 1: Define Schema and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afdf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power BI Metadata Schema\n",
    "powerbi_schema = StructType([\n",
    "    StructField(\"entity_id\", StringType(), False),\n",
    "    StructField(\"entity_type\", StringType(), False),  # dataset, report, dashboard, dataflow\n",
    "    StructField(\"entity_name\", StringType(), True),\n",
    "    StructField(\"workspace_id\", StringType(), True),\n",
    "    StructField(\"workspace_name\", StringType(), True),\n",
    "    StructField(\"created_datetime\", TimestampType(), True),\n",
    "    StructField(\"modified_datetime\", TimestampType(), True),\n",
    "    StructField(\"owner\", StringType(), True),\n",
    "    StructField(\"metadata_json\", StringType(), True),  # Full metadata as JSON\n",
    "    StructField(\"content_hash\", StringType(), True),   # Hash of content for change detection\n",
    "    StructField(\"ingestion_timestamp\", TimestampType(), False),\n",
    "    StructField(\"is_active\", BooleanType(), False)\n",
    "])\n",
    "\n",
    "# Purview Sync Log Schema\n",
    "sync_log_schema = StructType([\n",
    "    StructField(\"sync_id\", StringType(), False),\n",
    "    StructField(\"entity_id\", StringType(), False),\n",
    "    StructField(\"sync_timestamp\", TimestampType(), False),\n",
    "    StructField(\"sync_status\", StringType(), False),  # SUCCESS, FAILED, SKIPPED\n",
    "    StructField(\"change_type\", StringType(), False),  # INSERT, UPDATE, DELETE, NO_CHANGE\n",
    "    StructField(\"purview_asset_id\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"retry_count\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "def calculate_content_hash(row_dict):\n",
    "    \"\"\"Calculate hash of key fields to detect content changes\"\"\"\n",
    "    # Include fields that matter for Purview sync\n",
    "    key_fields = [\n",
    "        str(row_dict.get('entity_name', '')),\n",
    "        str(row_dict.get('workspace_name', '')),\n",
    "        str(row_dict.get('owner', '')),\n",
    "        str(row_dict.get('metadata_json', ''))\n",
    "    ]\n",
    "    content_str = '|'.join(key_fields)\n",
    "    return hashlib.sha256(content_str.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19856046",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Delta Tables (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e284c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_delta_tables():\n",
    "    \"\"\"Initialize Delta tables if they don't exist\"\"\"\n",
    "    \n",
    "    # Create Power BI Metadata table\n",
    "    if not DeltaTable.isDeltaTable(spark, POWERBI_METADATA_PATH):\n",
    "        print(\"Creating Power BI Metadata Delta table...\")\n",
    "        empty_df = spark.createDataFrame([], powerbi_schema)\n",
    "        (empty_df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"mergeSchema\", \"true\")\n",
    "         .save(POWERBI_METADATA_PATH))\n",
    "        print(f\"✓ Created table at {POWERBI_METADATA_PATH}\")\n",
    "    \n",
    "    # Create Purview Sync Log table\n",
    "    if not DeltaTable.isDeltaTable(spark, PURVIEW_SYNC_LOG_PATH):\n",
    "        print(\"Creating Purview Sync Log Delta table...\")\n",
    "        empty_df = spark.createDataFrame([], sync_log_schema)\n",
    "        (empty_df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(PURVIEW_SYNC_LOG_PATH))\n",
    "        print(f\"✓ Created table at {PURVIEW_SYNC_LOG_PATH}\")\n",
    "    \n",
    "    print(\"✓ Delta tables initialized\")\n",
    "\n",
    "# Run initialization\n",
    "initialize_delta_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb187de",
   "metadata": {},
   "source": [
    "## Step 3: Extract Power BI Data (Your API Function Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_powerbi_metadata():\n",
    "    \"\"\"\n",
    "    Extract Power BI metadata using your existing API function.\n",
    "    Replace this with your actual API call.\n",
    "    \"\"\"\n",
    "    # PLACEHOLDER: Replace with your actual Power BI REST API call\n",
    "    # Example structure of what your API might return:\n",
    "    # powerbi_data = your_powerbi_api_function()\n",
    "    \n",
    "    # For demonstration, here's the expected structure:\n",
    "    powerbi_raw_data = [\n",
    "        {\n",
    "            \"id\": \"dataset_123\",\n",
    "            \"type\": \"dataset\",\n",
    "            \"name\": \"Sales Dataset\",\n",
    "            \"workspaceId\": \"workspace_001\",\n",
    "            \"workspaceName\": \"Sales Workspace\",\n",
    "            \"createdDateTime\": \"2024-01-15T10:30:00Z\",\n",
    "            \"modifiedDateTime\": \"2024-11-20T14:22:00Z\",\n",
    "            \"configuredBy\": \"user@company.com\",\n",
    "            # ... other metadata fields from Power BI API\n",
    "        }\n",
    "        # ... more entities\n",
    "    ]\n",
    "    \n",
    "    # Transform to DataFrame with calculated hash\n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    transformed_data = []\n",
    "    for item in powerbi_raw_data:\n",
    "        row_dict = {\n",
    "            \"entity_id\": item.get(\"id\"),\n",
    "            \"entity_type\": item.get(\"type\"),\n",
    "            \"entity_name\": item.get(\"name\"),\n",
    "            \"workspace_id\": item.get(\"workspaceId\"),\n",
    "            \"workspace_name\": item.get(\"workspaceName\"),\n",
    "            \"created_datetime\": item.get(\"createdDateTime\"),\n",
    "            \"modified_datetime\": item.get(\"modifiedDateTime\"),\n",
    "            \"owner\": item.get(\"configuredBy\"),\n",
    "            \"metadata_json\": str(item),  # Store full metadata as JSON string\n",
    "            \"ingestion_timestamp\": current_timestamp,\n",
    "            \"is_active\": True\n",
    "        }\n",
    "        row_dict[\"content_hash\"] = calculate_content_hash(row_dict)\n",
    "        transformed_data.append(row_dict)\n",
    "    \n",
    "    df = spark.createDataFrame(transformed_data, powerbi_schema)\n",
    "    return df\n",
    "\n",
    "# Extract current Power BI metadata\n",
    "print(\"Extracting Power BI metadata...\")\n",
    "current_powerbi_df = extract_powerbi_metadata()\n",
    "print(f\"✓ Extracted {current_powerbi_df.count()} entities from Power BI\")\n",
    "current_powerbi_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae3f4d",
   "metadata": {},
   "source": [
    "## Step 4: CDC - Detect Changes Using Delta Lake Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cdc_merge(new_data_df):\n",
    "    \"\"\"\n",
    "    Perform CDC by merging new data with existing Delta table.\n",
    "    This identifies INSERTs, UPDATEs, and prepares for DELETE detection.\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, POWERBI_METADATA_PATH)\n",
    "    \n",
    "    # Perform merge operation\n",
    "    (delta_table.alias(\"target\")\n",
    "     .merge(\n",
    "         new_data_df.alias(\"source\"),\n",
    "         \"target.entity_id = source.entity_id\"\n",
    "     )\n",
    "     .whenMatchedUpdate(\n",
    "         condition = \"target.content_hash != source.content_hash OR target.modified_datetime != source.modified_datetime\",\n",
    "         set = {\n",
    "             \"entity_name\": \"source.entity_name\",\n",
    "             \"workspace_id\": \"source.workspace_id\",\n",
    "             \"workspace_name\": \"source.workspace_name\",\n",
    "             \"created_datetime\": \"source.created_datetime\",\n",
    "             \"modified_datetime\": \"source.modified_datetime\",\n",
    "             \"owner\": \"source.owner\",\n",
    "             \"metadata_json\": \"source.metadata_json\",\n",
    "             \"content_hash\": \"source.content_hash\",\n",
    "             \"ingestion_timestamp\": \"source.ingestion_timestamp\",\n",
    "             \"is_active\": \"source.is_active\"\n",
    "         }\n",
    "     )\n",
    "     .whenNotMatchedInsert(\n",
    "         values = {\n",
    "             \"entity_id\": \"source.entity_id\",\n",
    "             \"entity_type\": \"source.entity_type\",\n",
    "             \"entity_name\": \"source.entity_name\",\n",
    "             \"workspace_id\": \"source.workspace_id\",\n",
    "             \"workspace_name\": \"source.workspace_name\",\n",
    "             \"created_datetime\": \"source.created_datetime\",\n",
    "             \"modified_datetime\": \"source.modified_datetime\",\n",
    "             \"owner\": \"source.owner\",\n",
    "             \"metadata_json\": \"source.metadata_json\",\n",
    "             \"content_hash\": \"source.content_hash\",\n",
    "             \"ingestion_timestamp\": \"source.ingestion_timestamp\",\n",
    "             \"is_active\": \"source.is_active\"\n",
    "         }\n",
    "     )\n",
    "     .execute()\n",
    "    )\n",
    "    \n",
    "    print(\"✓ CDC merge completed\")\n",
    "\n",
    "# Perform CDC merge\n",
    "perform_cdc_merge(current_powerbi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211fa625",
   "metadata": {},
   "source": [
    "## Step 5: Identify Changes for Purview Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b54502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_changes_for_sync(lookback_hours=24):\n",
    "    \"\"\"\n",
    "    Identify entities that need to be synced to Purview.\n",
    "    Uses multiple strategies:\n",
    "    1. Recently modified (based on modified_datetime from Power BI API)\n",
    "    2. Recently ingested (new entities)\n",
    "    3. Failed previous syncs (for retry)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read current state from Delta table\n",
    "    metadata_df = spark.read.format(\"delta\").load(POWERBI_METADATA_PATH)\n",
    "    \n",
    "    # Read sync log to find last successful sync per entity\n",
    "    sync_log_df = spark.read.format(\"delta\").load(PURVIEW_SYNC_LOG_PATH)\n",
    "    \n",
    "    # Get last successful sync timestamp per entity\n",
    "    last_sync_df = (sync_log_df\n",
    "                    .filter(col(\"sync_status\") == \"SUCCESS\")\n",
    "                    .groupBy(\"entity_id\")\n",
    "                    .agg(max(\"sync_timestamp\").alias(\"last_sync_timestamp\")))\n",
    "    \n",
    "    # Join metadata with last sync info\n",
    "    entities_with_sync = (metadata_df\n",
    "                          .join(last_sync_df, \"entity_id\", \"left\"))\n",
    "    \n",
    "    # Identify changes using multiple conditions\n",
    "    cutoff_time = datetime.now() - timedelta(hours=lookback_hours)\n",
    "    \n",
    "    changes_df = entities_with_sync.filter(\n",
    "        # New entities (never synced)\n",
    "        (col(\"last_sync_timestamp\").isNull()) |\n",
    "        # Modified after last sync\n",
    "        (col(\"modified_datetime\") > col(\"last_sync_timestamp\")) |\n",
    "        # Recently ingested\n",
    "        (col(\"ingestion_timestamp\") > lit(cutoff_time))\n",
    "    ).select(\n",
    "        \"entity_id\",\n",
    "        \"entity_type\",\n",
    "        \"entity_name\",\n",
    "        \"workspace_id\",\n",
    "        \"workspace_name\",\n",
    "        \"metadata_json\",\n",
    "        \"modified_datetime\",\n",
    "        \"ingestion_timestamp\",\n",
    "        \"last_sync_timestamp\",\n",
    "        when(col(\"last_sync_timestamp\").isNull(), \"INSERT\")\n",
    "        .when(col(\"modified_datetime\") > col(\"last_sync_timestamp\"), \"UPDATE\")\n",
    "        .otherwise(\"INSERT\").alias(\"change_type\")\n",
    "    )\n",
    "    \n",
    "    # Also identify failed syncs for retry\n",
    "    failed_syncs_df = (sync_log_df\n",
    "                       .filter((col(\"sync_status\") == \"FAILED\") & (col(\"retry_count\") < 3))\n",
    "                       .groupBy(\"entity_id\")\n",
    "                       .agg(max(\"sync_timestamp\").alias(\"failed_timestamp\"),\n",
    "                            max(\"retry_count\").alias(\"retry_count\"))\n",
    "                       .join(metadata_df, \"entity_id\")\n",
    "                       .select(\n",
    "                           \"entity_id\",\n",
    "                           \"entity_type\",\n",
    "                           \"entity_name\",\n",
    "                           \"workspace_id\",\n",
    "                           \"workspace_name\",\n",
    "                           \"metadata_json\",\n",
    "                           \"modified_datetime\",\n",
    "                           \"ingestion_timestamp\",\n",
    "                           lit(None).cast(TimestampType()).alias(\"last_sync_timestamp\"),\n",
    "                           lit(\"RETRY\").alias(\"change_type\")\n",
    "                       ))\n",
    "    \n",
    "    # Union all changes\n",
    "    all_changes_df = changes_df.union(failed_syncs_df).distinct()\n",
    "    \n",
    "    return all_changes_df\n",
    "\n",
    "# Identify changes\n",
    "print(\"Identifying changes for Purview sync...\")\n",
    "changes_to_sync = identify_changes_for_sync(lookback_hours=24)\n",
    "print(f\"✓ Found {changes_to_sync.count()} entities to sync\")\n",
    "changes_to_sync.groupBy(\"change_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d08fee",
   "metadata": {},
   "source": [
    "## Step 6: Sync to Purview (Integration with Your API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def sync_entity_to_purview(entity_row):\n",
    "    \"\"\"\n",
    "    Sync a single entity to Purview using your existing API function.\n",
    "    Replace with your actual Purview API call.\n",
    "    \n",
    "    Returns: dict with sync result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PLACEHOLDER: Replace with your actual Purview API call\n",
    "        # Example: purview_response = your_purview_api_function(entity_row)\n",
    "        \n",
    "        # Simulate API call (replace this with your actual implementation)\n",
    "        purview_asset_id = f\"purview_{entity_row['entity_id']}\"\n",
    "        \n",
    "        # Your actual code would look something like:\n",
    "        # if entity_row['change_type'] == 'INSERT':\n",
    "        #     response = create_purview_entity(entity_row)\n",
    "        # elif entity_row['change_type'] == 'UPDATE':\n",
    "        #     response = update_purview_entity(entity_row)\n",
    "        \n",
    "        return {\n",
    "            \"sync_id\": str(uuid.uuid4()),\n",
    "            \"entity_id\": entity_row[\"entity_id\"],\n",
    "            \"sync_timestamp\": datetime.now(),\n",
    "            \"sync_status\": \"SUCCESS\",\n",
    "            \"change_type\": entity_row[\"change_type\"],\n",
    "            \"purview_asset_id\": purview_asset_id,\n",
    "            \"error_message\": None,\n",
    "            \"retry_count\": 0\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"sync_id\": str(uuid.uuid4()),\n",
    "            \"entity_id\": entity_row[\"entity_id\"],\n",
    "            \"sync_timestamp\": datetime.now(),\n",
    "            \"sync_status\": \"FAILED\",\n",
    "            \"change_type\": entity_row[\"change_type\"],\n",
    "            \"purview_asset_id\": None,\n",
    "            \"error_message\": str(e),\n",
    "            \"retry_count\": entity_row.get(\"retry_count\", 0) + 1\n",
    "        }\n",
    "\n",
    "def batch_sync_to_purview(changes_df, max_workers=5):\n",
    "    \"\"\"\n",
    "    Sync entities to Purview in parallel batches\n",
    "    \"\"\"\n",
    "    # Collect entities to sync (consider using iterators for very large datasets)\n",
    "    entities_to_sync = changes_df.collect()\n",
    "    \n",
    "    sync_results = []\n",
    "    \n",
    "    print(f\"Syncing {len(entities_to_sync)} entities to Purview...\")\n",
    "    \n",
    "    # Process in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_entity = {\n",
    "            executor.submit(sync_entity_to_purview, entity.asDict()): entity \n",
    "            for entity in entities_to_sync\n",
    "        }\n",
    "        \n",
    "        for i, future in enumerate(as_completed(future_to_entity), 1):\n",
    "            result = future.result()\n",
    "            sync_results.append(result)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Processed {i}/{len(entities_to_sync)} entities...\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    sync_log_df = spark.createDataFrame(sync_results, sync_log_schema)\n",
    "    \n",
    "    # Append to sync log Delta table\n",
    "    (sync_log_df.write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(PURVIEW_SYNC_LOG_PATH))\n",
    "    \n",
    "    # Print summary\n",
    "    success_count = sum(1 for r in sync_results if r[\"sync_status\"] == \"SUCCESS\")\n",
    "    failed_count = sum(1 for r in sync_results if r[\"sync_status\"] == \"FAILED\")\n",
    "    \n",
    "    print(f\"\\n✓ Sync completed:\")\n",
    "    print(f\"  - Success: {success_count}\")\n",
    "    print(f\"  - Failed: {failed_count}\")\n",
    "    \n",
    "    return sync_log_df\n",
    "\n",
    "# Perform sync\n",
    "if changes_to_sync.count() > 0:\n",
    "    sync_results = batch_sync_to_purview(changes_to_sync, max_workers=5)\n",
    "    sync_results.groupBy(\"sync_status\", \"change_type\").count().show()\n",
    "else:\n",
    "    print(\"No changes to sync\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe499585",
   "metadata": {},
   "source": [
    "## Step 7: Handle Deleted Entities (Soft Deletes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_deleted_entities(current_entities_df):\n",
    "    \"\"\"\n",
    "    Identify entities that exist in Delta table but not in current extract.\n",
    "    Mark them as inactive and optionally sync deletion to Purview.\n",
    "    \"\"\"\n",
    "    # Read existing active entities\n",
    "    existing_df = (spark.read.format(\"delta\")\n",
    "                   .load(POWERBI_METADATA_PATH)\n",
    "                   .filter(col(\"is_active\") == True)\n",
    "                   .select(\"entity_id\"))\n",
    "    \n",
    "    # Find entities in existing but not in current\n",
    "    current_ids_df = current_entities_df.select(\"entity_id\")\n",
    "    \n",
    "    deleted_entities_df = (existing_df\n",
    "                           .join(current_ids_df, \"entity_id\", \"left_anti\"))\n",
    "    \n",
    "    deleted_count = deleted_entities_df.count()\n",
    "    \n",
    "    if deleted_count > 0:\n",
    "        print(f\"Found {deleted_count} deleted entities\")\n",
    "        \n",
    "        # Update Delta table to mark as inactive\n",
    "        delta_table = DeltaTable.forPath(spark, POWERBI_METADATA_PATH)\n",
    "        \n",
    "        (delta_table.alias(\"target\")\n",
    "         .merge(\n",
    "             deleted_entities_df.alias(\"source\"),\n",
    "             \"target.entity_id = source.entity_id\"\n",
    "         )\n",
    "         .whenMatchedUpdate(\n",
    "             set = {\n",
    "                 \"is_active\": lit(False),\n",
    "                 \"ingestion_timestamp\": lit(datetime.now())\n",
    "             }\n",
    "         )\n",
    "         .execute())\n",
    "        \n",
    "        print(f\"✓ Marked {deleted_count} entities as inactive\")\n",
    "        \n",
    "        # Optionally sync deletions to Purview\n",
    "        # You can add logic here to delete or update status in Purview\n",
    "        \n",
    "    else:\n",
    "        print(\"No deleted entities found\")\n",
    "    \n",
    "    return deleted_entities_df\n",
    "\n",
    "# Handle deleted entities\n",
    "deleted_entities = handle_deleted_entities(current_powerbi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728e884",
   "metadata": {},
   "source": [
    "## Step 8: Monitoring and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sync history\n",
    "print(\"=== Recent Sync History ===\")\n",
    "recent_syncs = (spark.read.format(\"delta\")\n",
    "                .load(PURVIEW_SYNC_LOG_PATH)\n",
    "                .orderBy(col(\"sync_timestamp\").desc())\n",
    "                .limit(20))\n",
    "recent_syncs.show(truncate=False)\n",
    "\n",
    "# Sync success rate\n",
    "print(\"\\n=== Sync Success Rate (Last 24 Hours) ===\")\n",
    "last_24h = datetime.now() - timedelta(hours=24)\n",
    "sync_stats = (spark.read.format(\"delta\")\n",
    "              .load(PURVIEW_SYNC_LOG_PATH)\n",
    "              .filter(col(\"sync_timestamp\") > lit(last_24h))\n",
    "              .groupBy(\"sync_status\")\n",
    "              .count()\n",
    "              .orderBy(\"count\", ascending=False))\n",
    "sync_stats.show()\n",
    "\n",
    "# Entities by type\n",
    "print(\"\\n=== Entities by Type ===\")\n",
    "entity_stats = (spark.read.format(\"delta\")\n",
    "                .load(POWERBI_METADATA_PATH)\n",
    "                .filter(col(\"is_active\") == True)\n",
    "                .groupBy(\"entity_type\")\n",
    "                .count()\n",
    "                .orderBy(\"count\", ascending=False))\n",
    "entity_stats.show()\n",
    "\n",
    "# Failed syncs requiring attention\n",
    "print(\"\\n=== Failed Syncs Requiring Attention ===\")\n",
    "failed_syncs = (spark.read.format(\"delta\")\n",
    "                .load(PURVIEW_SYNC_LOG_PATH)\n",
    "                .filter((col(\"sync_status\") == \"FAILED\") & (col(\"retry_count\") >= 3))\n",
    "                .orderBy(col(\"sync_timestamp\").desc())\n",
    "                .limit(10))\n",
    "failed_syncs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d720d",
   "metadata": {},
   "source": [
    "## Step 9: Advanced - Time Travel Queries with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5723e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table history\n",
    "print(\"=== Delta Table History ===\")\n",
    "delta_table = DeltaTable.forPath(spark, POWERBI_METADATA_PATH)\n",
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(10, truncate=False)\n",
    "\n",
    "# Time travel - compare current vs yesterday\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Current data\n",
    "current_data = spark.read.format(\"delta\").load(POWERBI_METADATA_PATH)\n",
    "print(f\"\\nCurrent entity count: {current_data.count()}\")\n",
    "\n",
    "# Data as of yesterday (if available)\n",
    "try:\n",
    "    historical_data = (spark.read.format(\"delta\")\n",
    "                       .option(\"timestampAsOf\", yesterday.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                       .load(POWERBI_METADATA_PATH))\n",
    "    print(f\"Entity count yesterday: {historical_data.count()}\")\n",
    "    \n",
    "    # Compare changes\n",
    "    new_entities = (current_data.select(\"entity_id\")\n",
    "                   .subtract(historical_data.select(\"entity_id\")))\n",
    "    print(f\"New entities since yesterday: {new_entities.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Historical data not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c49c6",
   "metadata": {},
   "source": [
    "## Complete Orchestration Function\n",
    "\n",
    "Below is a complete function that orchestrates the entire CDC pipeline. You can schedule this to run periodically (e.g., via Databricks Jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cdc_pipeline(lookback_hours=24, max_workers=5):\n",
    "    \"\"\"\n",
    "    Complete CDC pipeline orchestration.\n",
    "    Run this function on a schedule (e.g., every hour or daily).\n",
    "    \n",
    "    Args:\n",
    "        lookback_hours: How far back to look for changes\n",
    "        max_workers: Number of parallel workers for Purview sync\n",
    "    \"\"\"\n",
    "    pipeline_start = datetime.now()\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Starting CDC Pipeline at {pipeline_start}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Initialize tables (idempotent)\n",
    "        print(\"Step 1: Initializing Delta tables...\")\n",
    "        initialize_delta_tables()\n",
    "        \n",
    "        # Step 2: Extract Power BI metadata\n",
    "        print(\"\\nStep 2: Extracting Power BI metadata...\")\n",
    "        current_data = extract_powerbi_metadata()\n",
    "        entity_count = current_data.count()\n",
    "        print(f\"✓ Extracted {entity_count} entities\")\n",
    "        \n",
    "        # Step 3: Perform CDC merge\n",
    "        print(\"\\nStep 3: Performing CDC merge...\")\n",
    "        perform_cdc_merge(current_data)\n",
    "        \n",
    "        # Step 4: Handle deletions\n",
    "        print(\"\\nStep 4: Checking for deleted entities...\")\n",
    "        handle_deleted_entities(current_data)\n",
    "        \n",
    "        # Step 5: Identify changes\n",
    "        print(f\"\\nStep 5: Identifying changes (lookback: {lookback_hours}h)...\")\n",
    "        changes = identify_changes_for_sync(lookback_hours)\n",
    "        change_count = changes.count()\n",
    "        print(f\"✓ Found {change_count} entities requiring sync\")\n",
    "        \n",
    "        # Step 6: Sync to Purview\n",
    "        if change_count > 0:\n",
    "            print(f\"\\nStep 6: Syncing to Purview (max workers: {max_workers})...\")\n",
    "            sync_results = batch_sync_to_purview(changes, max_workers)\n",
    "            \n",
    "            # Summary\n",
    "            success_count = sync_results.filter(col(\"sync_status\") == \"SUCCESS\").count()\n",
    "            failed_count = sync_results.filter(col(\"sync_status\") == \"FAILED\").count()\n",
    "        else:\n",
    "            print(\"\\nStep 6: No changes to sync\")\n",
    "            success_count = 0\n",
    "            failed_count = 0\n",
    "        \n",
    "        # Pipeline summary\n",
    "        pipeline_end = datetime.now()\n",
    "        duration = (pipeline_end - pipeline_start).total_seconds()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Pipeline completed successfully\")\n",
    "        print(f\"Duration: {duration:.2f} seconds\")\n",
    "        print(f\"Entities extracted: {entity_count}\")\n",
    "        print(f\"Changes detected: {change_count}\")\n",
    "        print(f\"Successful syncs: {success_count}\")\n",
    "        print(f\"Failed syncs: {failed_count}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"duration_seconds\": duration,\n",
    "            \"entities_extracted\": entity_count,\n",
    "            \"changes_detected\": change_count,\n",
    "            \"successful_syncs\": success_count,\n",
    "            \"failed_syncs\": failed_count\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_end = datetime.now()\n",
    "        duration = (pipeline_end - pipeline_start).total_seconds()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Pipeline FAILED after {duration:.2f} seconds\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"FAILED\",\n",
    "            \"duration_seconds\": duration,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Run the complete pipeline\n",
    "pipeline_result = run_cdc_pipeline(lookback_hours=24, max_workers=5)\n",
    "print(f\"\\nPipeline result: {pipeline_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d470691",
   "metadata": {},
   "source": [
    "## Best Practices & Optimization Tips\n",
    "\n",
    "### 1. **Scheduling Strategy**\n",
    "- **Incremental Updates**: Run every 1-6 hours for near real-time sync\n",
    "- **Full Reconciliation**: Weekly full scan to catch any missed changes\n",
    "- Use Databricks Jobs with retry policies\n",
    "\n",
    "### 2. **Performance Optimization**\n",
    "```python\n",
    "# Optimize Delta tables regularly\n",
    "spark.sql(f\"OPTIMIZE delta.`{POWERBI_METADATA_PATH}`\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{PURVIEW_SYNC_LOG_PATH}`\")\n",
    "\n",
    "# Vacuum old versions (keep 7 days)\n",
    "spark.sql(f\"VACUUM delta.`{POWERBI_METADATA_PATH}` RETAIN 168 HOURS\")\n",
    "```\n",
    "\n",
    "### 3. **Error Handling**\n",
    "- Implement exponential backoff for retries\n",
    "- Set up alerts for failed syncs (>3 retries)\n",
    "- Log detailed error messages\n",
    "\n",
    "### 4. **Monitoring**\n",
    "- Track sync latency and success rates\n",
    "- Monitor Delta table growth\n",
    "- Set up dashboards for data quality metrics\n",
    "\n",
    "### 5. **Data Quality Checks**\n",
    "- Validate entity_id uniqueness\n",
    "- Check for null values in critical fields\n",
    "- Monitor content_hash collisions\n",
    "\n",
    "### 6. **Leveraging Power BI API Metadata**\n",
    "The Power BI REST API provides:\n",
    "- `createdDateTime`: Use for INSERT detection\n",
    "- `modifiedDateTime`: Use for UPDATE detection  \n",
    "- `configuredBy`: Track ownership changes\n",
    "- Use these timestamps as your primary CDC signal!\n",
    "\n",
    "### 7. **Alternative CDC Strategies**\n",
    "\n",
    "**Option A: Timestamp-based (Current approach)**\n",
    "- ✅ Simple and reliable\n",
    "- ✅ Works with API metadata\n",
    "- ⚠️ Requires API to provide accurate timestamps\n",
    "\n",
    "**Option B: Hash-based only**\n",
    "- ✅ Detects any content changes\n",
    "- ⚠️ Higher compute cost (hash everything)\n",
    "- ⚠️ Can't distinguish between insert vs update\n",
    "\n",
    "**Option C: Hybrid (Recommended - What we implemented)**\n",
    "- ✅ Uses API timestamps + content hashing\n",
    "- ✅ Catches timestamp changes AND content changes\n",
    "- ✅ Most comprehensive change detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a51295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization - Run periodically\n",
    "def optimize_delta_tables():\n",
    "    \"\"\"Optimize Delta tables for better performance\"\"\"\n",
    "    print(\"Optimizing Delta tables...\")\n",
    "    \n",
    "    # Optimize (compacts small files)\n",
    "    spark.sql(f\"OPTIMIZE delta.`{POWERBI_METADATA_PATH}` ZORDER BY (entity_id, modified_datetime)\")\n",
    "    spark.sql(f\"OPTIMIZE delta.`{PURVIEW_SYNC_LOG_PATH}` ZORDER BY (entity_id, sync_timestamp)\")\n",
    "    \n",
    "    # Vacuum (removes old versions)\n",
    "    # spark.sql(f\"VACUUM delta.`{POWERBI_METADATA_PATH}` RETAIN 168 HOURS\")\n",
    "    # spark.sql(f\"VACUUM delta.`{PURVIEW_SYNC_LOG_PATH}` RETAIN 168 HOURS\")\n",
    "    \n",
    "    print(\"✓ Optimization complete\")\n",
    "\n",
    "# Uncomment to run optimization\n",
    "# optimize_delta_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814af9d3",
   "metadata": {},
   "source": [
    "## Summary: Key CDC Implementation Points\n",
    "\n",
    "### Architecture Overview\n",
    "```\n",
    "Power BI API → Extract → Delta Lake (Bronze) → CDC Logic → Purview API\n",
    "                                ↓\n",
    "                         Sync Log (Audit Trail)\n",
    "```\n",
    "\n",
    "### What You Need to Replace\n",
    "1. **`extract_powerbi_metadata()`** - Replace with your Power BI REST API call\n",
    "2. **`sync_entity_to_purview()`** - Replace with your Purview REST API call\n",
    "\n",
    "### Why This Approach Works\n",
    "1. **Delta Lake** provides:\n",
    "   - ACID transactions\n",
    "   - Time travel for auditing\n",
    "   - Efficient upserts (MERGE)\n",
    "   - Schema evolution\n",
    "\n",
    "2. **Hybrid CDC Strategy**:\n",
    "   - Uses Power BI API timestamps (`modifiedDateTime`, `createdDateTime`)\n",
    "   - Content hash for detecting changes not reflected in timestamps\n",
    "   - Sync log prevents duplicate processing\n",
    "\n",
    "3. **Scalability**:\n",
    "   - Parallel processing for Purview sync\n",
    "   - Incremental processing (only changed entities)\n",
    "   - Handles large datasets efficiently\n",
    "\n",
    "### Next Steps\n",
    "1. Replace placeholder API functions with your actual implementations\n",
    "2. Configure Delta table paths for your environment\n",
    "3. Set up Databricks Job for scheduled runs\n",
    "4. Configure monitoring and alerts\n",
    "5. Test with a small dataset first"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
